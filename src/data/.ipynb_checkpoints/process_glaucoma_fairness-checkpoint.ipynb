{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import cv2\n",
    "import random\n",
    "import sys \n",
    "import csv\n",
    "import pandas as pd\n",
    "import bz2\n",
    "from bz2 import BZ2File\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[2]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[3]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[4]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[5]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[6]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[7]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newscanfolder = '/shared/hdds_20T/yl535/elzelab/cirrus_ongoing/scans'\n",
    "newscanfolder_p2 = '/shared/external_hdds/external20TB_1/cirrus_from_dicom/scans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to mount eris data elzelab by<br><br>\n",
    "sshfs yl535@erisxdl.partners.org:/data/elzelab ~/elzelab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundus_meta_file = \"/shared/ssd_16T/meecs/ophai/dr/30.day.data.fundus.csv\"\n",
    "# fundus_img_dir = '/home/yanluo/external20TB_1/fundus_images/images'\n",
    "fundus_img_dir = '/shared/external_hdds/external20TB_1/fundus_images/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[12]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vffile = \"/shared/hdds_20T/yl535/elzelab/hfa_ongoing/hfa_ongoing_merged_24-2_subset_cleaned_all_in.csv\"<br>\n",
    "newoctfile = '/shared/ssd_16T/meecs/ophai/dr/30.day.data.oct.csv'<br>\n",
    "exclude_file = '/shared/ssd_16T/yl535/project/python/datasets/crosssectional/test1000_pidmaps.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_folder = '/shared/ssd_16T/yl535/project/python/datasets/havo_crosssectional_new_export/crosssectional_fairness_all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[13]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_folder = f'/shared/ssd_16T/yl535/project/python/datasets/harvard/glaucoma.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[14]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match_table_file = f'/shared/ssd_16T/meecs/ophai/dr/metadata_MacularCube_ongoing_cleaned.with.eye.specific.diagnosis.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_table_file = '/shared/ssd_16T/meecs/ophai/dr/30.day.data.oct.csv'\n",
    "save_to_folder = os.path.dirname(match_table_file)\n",
    "save_file_name = os.path.basename(match_table_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[15]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "samples_per_attr = [int(selected_patient_num/num_attr), int(selected_patient_num/num_attr), selected_patient_num-2*int(selected_patient_num/num_attr)]<br><br>\n",
    "samples_per_attr_test = [int(selected_patient_num_test/num_attr), int(selected_patient_num_test/num_attr), selected_patient_num_test-2*int(selected_patient_num_test/num_attr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/shared/hdds_20T/yl535/elzelab/cirrus_ongoing/scans/1.2.276.0.75.2.2.42.114374075547479.20191021140719035.3252403720<br><br>\n",
    "/shared/external_hdds/external20TB_1/cirrus_from_dicom/scans/1.2.276.0.75.2.2.42.114374075547479.20191021140719035.3252403720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_race = {1:'American Indian or Alaska Native', <br><br>\n",
    "                2:'Asian', <br><br>\n",
    "                3:'Black or African American', <br><br>\n",
    "                4:'Hispanic or Latino', <br><br>\n",
    "                5:'Native Hawaiian or Other Pacific Islander', <br><br>\n",
    "                6:'Other', <br><br>\n",
    "                7:'White or Caucasian'}<br><br>\n",
    "dict_race = {2:'Asian', <br><br>\n",
    "                3:'Black or African American', <br><br>\n",
    "                7:'White or Caucasian'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[16]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[17]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[18]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[19]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[20]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[21]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[22]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------start to process---------<br><br>\n",
    "get exluded patient list if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[23]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------process fundus-----------------<br><br><br>\n",
    "fundus_meta_data = csv.reader(open(fundus_meta_file), sep=',')<br><br><br>\n",
    "fundus_header = next(fundus_meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[130]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[24]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[131]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[32]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pprint(unique_imagetypes[count_sort_ind], log_output)<br><br>\n",
    "pprint(tmp_count[count_sort_ind], log_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------process oct-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[132]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[26]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[27]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[31]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[29]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[30]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "listdir: path should be string, bytes, os.PathLike, integer or None, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m file_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(input_folder)\n\u001b[1;32m     35\u001b[0m file_list\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# check only text files\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     40\u001b[0m         raw_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, f), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: listdir: path should be string, bytes, os.PathLike, integer or None, not list"
     ]
    }
   ],
   "source": [
    "input_folder = f'/data/home/luoy/project/python/datasets/havo_crosssectional_new_export/crosssectional_fairness_2100vs900_v1/train'\n",
    "meta_table = f'/data/home/luoy/project/python/datasets/havo_crosssectional_new_export/crosssectional_fairness_2100vs900/train_selected_patients_meta.csv'\n",
    "count_shift = 0\n",
    "\n",
    "# input_folder = f'/data/home/luoy/project/python/datasets/havo_crosssectional_new_export/crosssectional_fairness_2100vs900_v1/test'\n",
    "# meta_table = f'/data/home/luoy/project/python/datasets/havo_crosssectional_new_export/crosssectional_fairness_2100vs900/test_selected_patients_meta.csv'\n",
    "# count_shift = 2400\n",
    "\n",
    "output_folder = '/data/home/luoy/project/python/datasets/harvard/Harvard-GF'\n",
    "\n",
    "tmp_base = os.path.basename(input_folder)\n",
    "\n",
    "tmp_output_folder = os.path.join(output_folder, tmp_base)\n",
    "if not os.path.exists(tmp_output_folder):\n",
    "   os.makedirs(tmp_output_folder)\n",
    "\n",
    "meta_data = pd.read_csv(meta_table, sep=',')\n",
    "\n",
    "all_ages = []\n",
    "for index, row in meta_data.iterrows():\n",
    "    pid = str(row[0])\n",
    "    male = row['male']\n",
    "    if np.isnan(male):\n",
    "        continue\n",
    "    male = int(male)\n",
    "    age = row['age']\n",
    "    if np.isnan(age):\n",
    "        age = -1\n",
    "    age = age/365\n",
    "    all_ages.append(age)\n",
    "\n",
    "count = 1\n",
    "\n",
    "file_list = os.listdir(input_folder)\n",
    "file_list.sort()\n",
    "\n",
    "for f in file_list:\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "        raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        file_to_save = f'data_{count+count_shift:04d}.npz'\n",
    "#         print(f)\n",
    "        np.savez(os.path.join(tmp_output_folder,file_to_save), \n",
    "            oct_bscans=raw_data['oct_bscans'],\n",
    "            rnflt=raw_data['rnflt'],\n",
    "            md=raw_data['md'],\n",
    "            glaucoma=raw_data['glaucoma'],\n",
    "            tds=raw_data['tds'],\n",
    "            race=raw_data['race'],\n",
    "            male=raw_data['male'],\n",
    "            hispanic=raw_data['hispanic'],\n",
    "            language=raw_data['language'],\n",
    "            maritalstatus=raw_data['maritalstatus'],\n",
    "            age=all_ages[count-1])\n",
    "        \n",
    "        count += 1\n",
    "print(f'count = {count+count_shift}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 2401\n"
     ]
    }
   ],
   "source": [
    "input_folder = f'/data/home/luoy/project/python/datasets/havo_crosssectional_new_export/crosssectional_fairness_2100vs900_v1/val'\n",
    "count_shift = 2100\n",
    "\n",
    "output_folder = '/data/home/luoy/project/python/datasets/harvard/Harvard-GF'\n",
    "\n",
    "tmp_base = os.path.basename(input_folder)\n",
    "\n",
    "tmp_output_folder = os.path.join(output_folder, tmp_base)\n",
    "if not os.path.exists(tmp_output_folder):\n",
    "   os.makedirs(tmp_output_folder)\n",
    "\n",
    "count = 1\n",
    "\n",
    "file_list = os.listdir(input_folder)\n",
    "file_list.sort()\n",
    "\n",
    "for f in file_list:\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "#         raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        file_to_save = f'data_{count+count_shift:04d}.npz'\n",
    "        \n",
    "        shutil.copy(os.path.join(input_folder, f), os.path.join(tmp_output_folder, file_to_save))\n",
    "        \n",
    "        count += 1\n",
    "print(f'count = {count+count_shift}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number: 3300\n"
     ]
    }
   ],
   "source": [
    "data_folder = f'/data/home/luoy/project/python/datasets/harvard/Harvard-GF'\n",
    "output_folder = f'/data/home/luoy/project/python/datasets/harvard/Harvard-GF'\n",
    "output_csv = 'demographics_diagnosis.csv'\n",
    "\n",
    "all_meta = []\n",
    "count = 1\n",
    "\n",
    "input_folder = os.path.join(data_folder, 'train')\n",
    "tmp_base = os.path.basename(input_folder)\n",
    "\n",
    "fundus_files = []\n",
    "data_files = []\n",
    "tmp_output_folder = os.path.join(output_folder, tmp_base)\n",
    "if not os.path.exists(tmp_output_folder):\n",
    "   os.makedirs(tmp_output_folder)\n",
    "for f in os.listdir(input_folder):\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "        raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        \n",
    "#         file_to_save = f'{tmp_base}_data_{count:04d}.npz'\n",
    "\n",
    "        all_meta.append(f\"{f}, {raw_data['glaucoma'].item()}, \\\n",
    "            {raw_data['age'].item():.2f}, {raw_data['race'].item()}, {raw_data['male'].item()}, \\\n",
    "            {raw_data['hispanic'].item()}, {raw_data['maritalstatus'].item()}, {raw_data['language'].item()}\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "input_folder = os.path.join(data_folder, 'val')\n",
    "tmp_base = os.path.basename(input_folder)\n",
    "tmp_output_folder = os.path.join(output_folder, tmp_base)\n",
    "if not os.path.exists(tmp_output_folder):\n",
    "   os.makedirs(tmp_output_folder)\n",
    "\n",
    "fundus_files = []\n",
    "data_files = []\n",
    "for f in os.listdir(input_folder):\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "        raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        \n",
    "#         file_to_save = f'{tmp_base}_data_{count:04d}.npz'\n",
    "\n",
    "\n",
    "        all_meta.append(f\"{f}, {raw_data['glaucoma'].item()}, \\\n",
    "            {raw_data['age'].item():.2f}, {raw_data['race'].item()}, {raw_data['male'].item()}, \\\n",
    "            {raw_data['hispanic'].item()}, {raw_data['maritalstatus'].item()}, {raw_data['language'].item()}\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "input_folder = os.path.join(data_folder, 'test')\n",
    "tmp_base = os.path.basename(input_folder)\n",
    "tmp_output_folder = os.path.join(output_folder, tmp_base)\n",
    "if not os.path.exists(tmp_output_folder):\n",
    "   os.makedirs(tmp_output_folder)\n",
    "\n",
    "fundus_files = []\n",
    "data_files = []\n",
    "for f in os.listdir(input_folder):\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "        raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        \n",
    "#         file_to_save = f'{tmp_base}_data_{count:04d}.npz'\n",
    "\n",
    "        all_meta.append(f\"{f}, {raw_data['glaucoma'].item()}, \\\n",
    "            {raw_data['age'].item():.2f}, {raw_data['race'].item()}, {raw_data['male'].item()}, \\\n",
    "            {raw_data['hispanic'].item()}, {raw_data['maritalstatus'].item()}, {raw_data['language'].item()}\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "print(f'total number: {count-1}')\n",
    "with open(os.path.join(output_folder, output_csv), 'w') as f:\n",
    "    f.write(f'Filename, Glaucoma, Age, Race, IsMale, IsHispanic, MaritalStatus, Language\\n')\n",
    "    for x in all_meta:\n",
    "        f.write(f'{x}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rnflt', 'oct_bscans', 'md', 'tds', 'race', 'male', 'glaucoma', 'language', 'hispanic', 'maritalstatus']\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_folder = f'/data/home/luoy/project/python/datasets/havo_crosssectional_new_export/crosssectional_fairness_2100vs900_v1'\n",
    "output_folder = f'/data/home/luoy/project/python/datasets/harvard/Harvard_GF'\n",
    "output_csv = 'demographics_diagnosis.csv'\n",
    "\n",
    "count = 1\n",
    "all_meta = []\n",
    "# if not os.path.exists(output_folder):\n",
    "#    os.makedirs(output_folder)\n",
    "\n",
    "input_folder = os.path.join(data_folder, 'train')\n",
    "tmp_base = os.path.basename(input_folder)\n",
    "\n",
    "fundus_files = []\n",
    "data_files = []\n",
    "tmp_output_folder = os.path.join(output_folder, tmp_base)\n",
    "if not os.path.exists(tmp_output_folder):\n",
    "   os.makedirs(tmp_output_folder)\n",
    "for f in os.listdir(input_folder):\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "        raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        \n",
    "        file_to_save = f'{tmp_base}_data_{count:04d}.npz'\n",
    "        shutil.copy(os.path.join(input_folder, f), os.path.join(tmp_output_folder, file_to_save))\n",
    "        \n",
    "#         np.savez(os.path.join(tmp_output_folder,file_to_save), \n",
    "#                     oct_bscans=raw_data['oct_bscans'],\n",
    "#                     fundus=raw_data['fundus'],\n",
    "#                     race=raw_data['race'].item(),\n",
    "#                     male=raw_data['male'].item(),\n",
    "#                     hispanic=raw_data['hispanic'].item(),\n",
    "#                     age=raw_data['age'].item(),\n",
    "#                     dr_class=raw_data['dr_class'].item())\n",
    "        all_meta.append(f\"{file_to_save}, {raw_data['glaucoma'].item()}, \\\n",
    "            {raw_data['race'].item()}, {raw_data['male'].item()}, \\\n",
    "            {raw_data['hispanic'].item()}, {raw_data['maritalstatus'].item()}, {raw_data['language'].item()}\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "input_folder = os.path.join(data_folder, 'val')\n",
    "tmp_base = os.path.basename(input_folder)\n",
    "tmp_output_folder = os.path.join(output_folder, tmp_base)\n",
    "if not os.path.exists(tmp_output_folder):\n",
    "   os.makedirs(tmp_output_folder)\n",
    "\n",
    "fundus_files = []\n",
    "data_files = []\n",
    "for f in os.listdir(input_folder):\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "        raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        \n",
    "        file_to_save = f'{tmp_base}_data_{count:04d}.npz'\n",
    "        shutil.copy(os.path.join(input_folder, f), os.path.join(tmp_output_folder, file_to_save))\n",
    "                 \n",
    "#         np.savez(os.path.join(tmp_output_folder, file_to_save), \n",
    "#                     oct_bscans=raw_data['oct_bscans'],\n",
    "#                     fundus=raw_data['fundus'],\n",
    "#                     race=raw_data['race'].item(),\n",
    "#                     male=raw_data['male'].item(),\n",
    "#                     hispanic=raw_data['hispanic'].item(),\n",
    "#                     age=raw_data['age'].item(),\n",
    "#                     dr_class=raw_data['dr_class'].item())\n",
    "\n",
    "        all_meta.append(f\"{file_to_save}, {raw_data['glaucoma'].item()}, \\\n",
    "            {raw_data['race'].item()}, {raw_data['male'].item()}, \\\n",
    "            {raw_data['hispanic'].item()}, {raw_data['maritalstatus'].item()}, {raw_data['language'].item()}\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "input_folder = os.path.join(data_folder, 'test')\n",
    "tmp_base = os.path.basename(input_folder)\n",
    "tmp_output_folder = os.path.join(output_folder, tmp_base)\n",
    "if not os.path.exists(tmp_output_folder):\n",
    "   os.makedirs(tmp_output_folder)\n",
    "\n",
    "fundus_files = []\n",
    "data_files = []\n",
    "for f in os.listdir(input_folder):\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "        raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        \n",
    "        file_to_save = f'{tmp_base}_data_{count:04d}.npz'\n",
    "        shutil.copy(os.path.join(input_folder, f), os.path.join(tmp_output_folder, file_to_save))\n",
    "                 \n",
    "#         np.savez(os.path.join(tmp_output_folder, file_to_save), \n",
    "#                     oct_bscans=raw_data['oct_bscans'],\n",
    "#                     fundus=raw_data['fundus'],\n",
    "#                     race=raw_data['race'].item(),\n",
    "#                     male=raw_data['male'].item(),\n",
    "#                     hispanic=raw_data['hispanic'].item(),\n",
    "#                     age=raw_data['age'].item(),\n",
    "#                     dr_class=raw_data['dr_class'].item())\n",
    "        all_meta.append(f\"{file_to_save}, {raw_data['glaucoma'].item()}, \\\n",
    "            {raw_data['race'].item()}, {raw_data['male'].item()}, \\\n",
    "            {raw_data['hispanic'].item()}, {raw_data['maritalstatus'].item()}, {raw_data['language'].item()}\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "print(f'total number: {count-1}')\n",
    "with open(os.path.join(output_folder, output_csv), 'w') as f:\n",
    "    f.write(f'Filename, Glaucoma, Race, IsMale, IsHispanic, MaritalStatus, Language\\n')\n",
    "    for x in all_meta:\n",
    "        f.write(f'{x}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rnflt', 'oct_bscans', 'md', 'tds', 'race', 'male', 'glaucoma']\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_folder = f'/data/home/luoy/project/python/datasets/havo_crosssectional_new_export/crosssectional_fairness_2100vs900'\n",
    "input_folder = os.path.join(data_folder, 'train')\n",
    "for f in os.listdir(input_folder):\n",
    "    # check only text files\n",
    "    if f.endswith('.npz'):\n",
    "        raw_data = np.load(os.path.join(input_folder, f), allow_pickle=True)\n",
    "        print(raw_data.files)\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
